{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT_LN_tweets_for_anger.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ZhkhIRtFigCN","colab_type":"code","colab":{}},"source":["# This modeling code is based on the tutorial script here:\n","# https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JajIyFa0Ac-_","colab_type":"code","outputId":"d095a8a7-fb85-4a06-8167-65adb9b441b9","executionInfo":{"status":"ok","timestamp":1586841173298,"user_tz":420,"elapsed":8921,"user":{"displayName":"Zichuang Xie","photoUrl":"","userId":"08982348595392357027"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1LlRmAt1Aeja","colab_type":"code","outputId":"04048633-4756-4b0e-e5ad-5056860ae67a","executionInfo":{"status":"ok","timestamp":1586841376336,"user_tz":420,"elapsed":4100,"user":{"displayName":"Zichuang Xie","photoUrl":"","userId":"08982348595392357027"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hvXQHVT4A2Av","colab_type":"code","colab":{}},"source":["!pip install transformers\n","import numpy as np\n","import pandas as pd\n","import os"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dbcieC9nA5Hp","colab_type":"code","colab":{}},"source":["# Load the dataset into a pandas dataframe.\n","df = pd.read_csv(\"/content/training_data_for_anger.csv\")\n","\n","# Report the number of sentences.\n","print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n","\n","# Display 10 random rows from the data.\n","df.sample(10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uhkKreHjBkXK","colab_type":"code","colab":{}},"source":["anger = df[['clean_text','anger']]\n","anger.anger = [np.nan_to_num(x) for x in anger['anger']]\n","anger = anger.astype({\"anger\": int})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mfs0yaLVCIc5","colab_type":"code","colab":{}},"source":["anger.sample(10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jl-Rly_XMob0","colab_type":"code","colab":{}},"source":["# Get the lists of sentences and their labels.\n","sentences = anger.clean_text.values\n","labels = anger.anger.values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ErbX-5iNhcb","colab_type":"code","outputId":"08f2955b-275d-413d-c271-041e36f27d18","executionInfo":{"status":"ok","timestamp":1586851588057,"user_tz":420,"elapsed":666,"user":{"displayName":"Zichuang Xie","photoUrl":"","userId":"08982348595392357027"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loading BERT tokenizer...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ry_n3qifNnXM","colab_type":"code","outputId":"2d63f052-e3ed-4154-ab35-5923eedb652e","executionInfo":{"status":"ok","timestamp":1586851590820,"user_tz":420,"elapsed":292,"user":{"displayName":"Zichuang Xie","photoUrl":"","userId":"08982348595392357027"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["# Print the original sentence.\n","print(' Original: ', sentences[0])\n","\n","# Print the sentence split into tokens.\n","print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n","\n","# Print the sentence mapped to token ids.\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" Original:  Question: who is going to vote for Biden in the general election after all his elderly voters expose themselves to Coronavirus during the primary? \n","Tokenized:  ['question', ':', 'who', 'is', 'going', 'to', 'vote', 'for', 'bid', '##en', 'in', 'the', 'general', 'election', 'after', 'all', 'his', 'elderly', 'voters', 'expose', 'themselves', 'to', 'corona', '##virus', 'during', 'the', 'primary', '?']\n","Token IDs:  [3160, 1024, 2040, 2003, 2183, 2000, 3789, 2005, 7226, 2368, 1999, 1996, 2236, 2602, 2044, 2035, 2010, 9750, 7206, 14451, 3209, 2000, 21887, 23350, 2076, 1996, 3078, 1029]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l6w8elb-58GJ","colab_type":"text"},"source":["## Sentences to IDs"]},{"cell_type":"code","metadata":{"id":"jUBPDnPmNp9p","colab_type":"code","outputId":"abb17676-f7c1-4515-f2b6-7f1fd818f2c5","executionInfo":{"status":"ok","timestamp":1586851601088,"user_tz":420,"elapsed":7406,"user":{"displayName":"Zichuang Xie","photoUrl":"","userId":"08982348595392357027"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","\n","# For every sentence...\n","for sent in sentences:\n","    encoded_sent = tokenizer.encode(\n","                        sent,  # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","\n","                              )\n","    \n","    # Add the encoded sentence to the list.\n","    input_ids.append(encoded_sent)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', sentences[0])\n","print('Token IDs:', input_ids[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Original:  Question: who is going to vote for Biden in the general election after all his elderly voters expose themselves to Coronavirus during the primary? \n","Token IDs: [101, 3160, 1024, 2040, 2003, 2183, 2000, 3789, 2005, 7226, 2368, 1999, 1996, 2236, 2602, 2044, 2035, 2010, 9750, 7206, 14451, 3209, 2000, 21887, 23350, 2076, 1996, 3078, 1029, 102]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lYWEQk5nObfD","colab_type":"text"},"source":["## Padding & Truncating"]},{"cell_type":"code","metadata":{"id":"u9-KLCGmN6xB","colab_type":"code","outputId":"988509f0-9ff8-48b6-b7d8-5f158a50fcce","executionInfo":{"status":"ok","timestamp":1586851627989,"user_tz":420,"elapsed":453,"user":{"displayName":"Zichuang Xie","photoUrl":"","userId":"08982348595392357027"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print('Max sentence length: ', max([len(sen) for sen in input_ids]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Max sentence length:  186\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TPqkrKZkOCI_","colab_type":"code","outputId":"16191236-ad7d-4fda-edca-3b87792140e7","executionInfo":{"status":"ok","timestamp":1586851634395,"user_tz":420,"elapsed":385,"user":{"displayName":"Zichuang Xie","photoUrl":"","userId":"08982348595392357027"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["# We'll borrow the `pad_sequences` utility function to do this.\n","from keras.preprocessing.sequence import pad_sequences\n","\n","# Set the maximum sequence length.\n","MAX_LEN = 200\n","\n","print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n","\n","print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n","\n","# Pad our input tokens with value 0.\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n","                          value=0, truncating=\"post\", padding=\"post\")\n","\n","print('\\nDone.')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","Padding/truncating all sentences to 200 values...\n","\n","Padding token: \"[PAD]\", ID: 0\n","\n","Done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LGJG2x8EOidw","colab_type":"text"},"source":["## Attention Masks"]},{"cell_type":"code","metadata":{"id":"PHZbsYeSOQoC","colab_type":"code","colab":{}},"source":["# Create attention masks\n","attention_masks = []\n","\n","# For each sentence...\n","for sent in input_ids:\n","    \n","    # Create the attention mask.\n","    att_mask = [int(token_id > 0) for token_id in sent]\n","    \n","    # Store the attention mask for this sentence.\n","    attention_masks.append(att_mask)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AUMyoyvHOo9H","colab_type":"text"},"source":["## Train/validation Split"]},{"cell_type":"code","metadata":{"id":"TDHA8AcxOmK4","colab_type":"code","colab":{}},"source":["# Use train_test_split to split our data into train and validation sets for\n","# training\n","from sklearn.model_selection import train_test_split\n","\n","# Use 90% for training and 10% for validation.\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n","                                            random_state=1999, test_size=0.1)\n","# Do the same for the masks.\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n","                              random_state=1999, test_size=0.1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pcVP5QjoPTd-","colab_type":"text"},"source":["## Converting to Pytorch Form"]},{"cell_type":"code","metadata":{"id":"aqXY3OPbPNSm","colab_type":"code","colab":{}},"source":["# Convert all inputs and labels into torch tensors, the required datatype.\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xf82JfnZPZOm","colab_type":"code","colab":{}},"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","batch_size = 16\n","\n","# Create the DataLoader for our training set.\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set.\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v33eEBtpPg41","colab_type":"text"},"source":["# Modeling"]},{"cell_type":"code","metadata":{"id":"D9TuWSa3PefG","colab_type":"code","colab":{}},"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 2,    \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","model.cuda()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TnneE0Q4QFD_","colab_type":"text"},"source":["## Set Optimizer and Learning Rate"]},{"cell_type":"code","metadata":{"id":"ecsQWY-XPnvR","colab_type":"code","colab":{}},"source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr = 5e-5, # args.learning_rate \n","                  eps = 1e-8 # args.adam_epsilon  \n","                )\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w8jM1XwrP9R4","colab_type":"code","colab":{}},"source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs (authors recommend between 2 and 4)\n","epochs = 4\n","\n","# Total number of training steps is number of batches * number of epochs.\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                       num_warmup_steps = 0, # Default value in run_glue.py\n","                       num_training_steps = total_steps)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h7hl2qYVQKXC","colab_type":"text"},"source":["## Training Loop"]},{"cell_type":"code","metadata":{"id":"MFFrud2NQBm9","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ekKg0-2FQN_w","colab_type":"code","colab":{}},"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FxAt4zFNQRVG","colab_type":"code","outputId":"61bd7e8f-7e8d-4eab-ac8e-b8b3a9f46cb8","executionInfo":{"status":"ok","timestamp":1586853800776,"user_tz":420,"elapsed":764692,"user":{"displayName":"Zichuang Xie","photoUrl":"","userId":"08982348595392357027"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import random\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 18\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Store the average loss after each epoch so we can plot them.\n","loss_values = []\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # This will return the loss (rather than the model output) because we\n","        # have provided the `labels`.\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        outputs = model(b_input_ids, \n","                    token_type_ids=None, \n","                    attention_mask=b_input_mask, \n","                    labels=b_labels)\n","        \n","        # The call to `model` always returns a tuple, so we need to pull the \n","        # loss value out of the tuple.\n","        loss = outputs[0]\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","    \n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # Telling the model not to compute or store gradients, saving memory and\n","        # speeding up validation\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # This will return the logits rather than the loss because we have\n","            # not provided labels.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # Get the \"logits\" output by the model. The \"logits\" are the output\n","        # values prior to applying an activation function like the softmax.\n","        logits = outputs[0]\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # Calculate the accuracy for this batch of test sentences.\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        \n","        # Accumulate the total accuracy.\n","        eval_accuracy += tmp_eval_accuracy\n","\n","        # Track the number of batches\n","        nb_eval_steps += 1\n","\n","    # Report the final accuracy for this validation run.\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of    768.    Elapsed: 0:00:26.\n","  Batch    80  of    768.    Elapsed: 0:00:52.\n","  Batch   120  of    768.    Elapsed: 0:01:19.\n","  Batch   160  of    768.    Elapsed: 0:01:45.\n","  Batch   200  of    768.    Elapsed: 0:02:12.\n","  Batch   240  of    768.    Elapsed: 0:02:39.\n","  Batch   280  of    768.    Elapsed: 0:03:06.\n","  Batch   320  of    768.    Elapsed: 0:03:33.\n","  Batch   360  of    768.    Elapsed: 0:04:00.\n","  Batch   400  of    768.    Elapsed: 0:04:26.\n","  Batch   440  of    768.    Elapsed: 0:04:53.\n","  Batch   480  of    768.    Elapsed: 0:05:20.\n","  Batch   520  of    768.    Elapsed: 0:05:47.\n","  Batch   560  of    768.    Elapsed: 0:06:14.\n","  Batch   600  of    768.    Elapsed: 0:06:41.\n","  Batch   640  of    768.    Elapsed: 0:07:08.\n","  Batch   680  of    768.    Elapsed: 0:07:35.\n","  Batch   720  of    768.    Elapsed: 0:08:01.\n","  Batch   760  of    768.    Elapsed: 0:08:28.\n","\n","  Average training loss: 0.50\n","  Training epcoh took: 0:08:33\n","\n","Running Validation...\n","  Accuracy: 0.85\n","  Validation took: 0:00:18\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of    768.    Elapsed: 0:00:27.\n","  Batch    80  of    768.    Elapsed: 0:00:54.\n","  Batch   120  of    768.    Elapsed: 0:01:21.\n","  Batch   160  of    768.    Elapsed: 0:01:48.\n","  Batch   200  of    768.    Elapsed: 0:02:14.\n","  Batch   240  of    768.    Elapsed: 0:02:41.\n","  Batch   280  of    768.    Elapsed: 0:03:08.\n","  Batch   320  of    768.    Elapsed: 0:03:35.\n","  Batch   360  of    768.    Elapsed: 0:04:02.\n","  Batch   400  of    768.    Elapsed: 0:04:29.\n","  Batch   440  of    768.    Elapsed: 0:04:56.\n","  Batch   480  of    768.    Elapsed: 0:05:23.\n","  Batch   520  of    768.    Elapsed: 0:05:49.\n","  Batch   560  of    768.    Elapsed: 0:06:16.\n","  Batch   600  of    768.    Elapsed: 0:06:43.\n","  Batch   640  of    768.    Elapsed: 0:07:10.\n","  Batch   680  of    768.    Elapsed: 0:07:37.\n","  Batch   720  of    768.    Elapsed: 0:08:04.\n","  Batch   760  of    768.    Elapsed: 0:08:31.\n","\n","  Average training loss: 0.26\n","  Training epcoh took: 0:08:36\n","\n","Running Validation...\n","  Accuracy: 0.87\n","  Validation took: 0:00:18\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of    768.    Elapsed: 0:00:27.\n","  Batch    80  of    768.    Elapsed: 0:00:54.\n","  Batch   120  of    768.    Elapsed: 0:01:20.\n","  Batch   160  of    768.    Elapsed: 0:01:47.\n","  Batch   200  of    768.    Elapsed: 0:02:14.\n","  Batch   240  of    768.    Elapsed: 0:02:41.\n","  Batch   280  of    768.    Elapsed: 0:03:08.\n","  Batch   320  of    768.    Elapsed: 0:03:35.\n","  Batch   360  of    768.    Elapsed: 0:04:02.\n","  Batch   400  of    768.    Elapsed: 0:04:29.\n","  Batch   440  of    768.    Elapsed: 0:04:55.\n","  Batch   480  of    768.    Elapsed: 0:05:22.\n","  Batch   520  of    768.    Elapsed: 0:05:49.\n","  Batch   560  of    768.    Elapsed: 0:06:16.\n","  Batch   600  of    768.    Elapsed: 0:06:43.\n","  Batch   640  of    768.    Elapsed: 0:07:10.\n","  Batch   680  of    768.    Elapsed: 0:07:37.\n","  Batch   720  of    768.    Elapsed: 0:08:04.\n","  Batch   760  of    768.    Elapsed: 0:08:30.\n","\n","  Average training loss: 0.14\n","  Training epcoh took: 0:08:36\n","\n","Running Validation...\n","  Accuracy: 0.88\n","  Validation took: 0:00:18\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of    768.    Elapsed: 0:00:27.\n","  Batch    80  of    768.    Elapsed: 0:00:54.\n","  Batch   120  of    768.    Elapsed: 0:01:20.\n","  Batch   160  of    768.    Elapsed: 0:01:47.\n","  Batch   200  of    768.    Elapsed: 0:02:14.\n","  Batch   240  of    768.    Elapsed: 0:02:41.\n","  Batch   280  of    768.    Elapsed: 0:03:07.\n","  Batch   320  of    768.    Elapsed: 0:03:34.\n","  Batch   360  of    768.    Elapsed: 0:04:01.\n","  Batch   400  of    768.    Elapsed: 0:04:28.\n","  Batch   440  of    768.    Elapsed: 0:04:55.\n","  Batch   480  of    768.    Elapsed: 0:05:21.\n","  Batch   520  of    768.    Elapsed: 0:05:48.\n","  Batch   560  of    768.    Elapsed: 0:06:15.\n","  Batch   600  of    768.    Elapsed: 0:06:42.\n","  Batch   640  of    768.    Elapsed: 0:07:09.\n","  Batch   680  of    768.    Elapsed: 0:07:35.\n","  Batch   720  of    768.    Elapsed: 0:08:02.\n","  Batch   760  of    768.    Elapsed: 0:08:29.\n","\n","  Average training loss: 0.06\n","  Training epcoh took: 0:08:34\n","\n","Running Validation...\n","  Accuracy: 0.89\n","  Validation took: 0:00:18\n","\n","Training complete!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ophvlNgxSuNT","colab_type":"text"},"source":["## Plot Loss"]},{"cell_type":"code","metadata":{"id":"-RQMCrQEQaCF","colab_type":"code","outputId":"cdb4b3d5-ec02-46f9-bc71-5fb580a82f2a","executionInfo":{"status":"ok","timestamp":1586842434749,"user_tz":420,"elapsed":953,"user":{"displayName":"Zichuang Xie","photoUrl":"","userId":"08982348595392357027"}},"colab":{"base_uri":"https://localhost:8080/","height":482}},"source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","import seaborn as sns\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(loss_values, 'b-o')\n","\n","# Label the plot.\n","plt.title(\"Training loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x432 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"GY6SptSQZZ1l","colab_type":"text"},"source":["# Perform on Test"]},{"cell_type":"code","metadata":{"id":"-KaJNZmgaOp4","colab_type":"code","outputId":"4834550e-596b-4b1d-9ffc-10e3c2f55118","executionInfo":{"status":"ok","timestamp":1586842606758,"user_tz":420,"elapsed":650,"user":{"displayName":"Zichuang Xie","photoUrl":"","userId":"08982348595392357027"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["import pandas as pd\n","\n","# Load the dataset into a pandas dataframe.\n","df = pd.read_csv(\"/content/test_data_2.csv\")\n","\n","# Report the number of sentences.\n","print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n","anger = df[['clean_text','anger']]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of test sentences: 12,000\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4TD4Z4NwaSf2","colab_type":"code","outputId":"6f82c750-557d-4a8b-d651-84226719bdea","executionInfo":{"status":"ok","timestamp":1586842622048,"user_tz":420,"elapsed":659,"user":{"displayName":"Zichuang Xie","photoUrl":"","userId":"08982348595392357027"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["anger.anger = [np.nan_to_num(x) for x in anger['anger']]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  self[name] = value\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"nLpA6e1zaQIl","colab_type":"code","colab":{}},"source":["anger = anger.astype({\"anger\": int})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"D4raPvx7Tg_K","colab_type":"code","colab":{}},"source":["# Create sentence and label lists\n","sentences = anger.clean_text.values\n","labels = anger.anger.values\n","\n","# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","\n","# For every sentence...\n","for sent in sentences:\n","    # `encode` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    encoded_sent = tokenizer.encode(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                   )\n","    \n","    input_ids.append(encoded_sent)\n","\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n","                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","# Convert to tensors.\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","\n","# Set the batch size.  \n","batch_size = 16  \n","\n","# Create the DataLoader.\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ey3Q3i1VaETC","colab_type":"code","outputId":"0a150d68-5b6d-4a8c-a8bf-ca05e85e64fe","executionInfo":{"status":"ok","timestamp":1586842846354,"user_tz":420,"elapsed":117126,"user":{"displayName":"Zichuang Xie","photoUrl":"","userId":"08982348595392357027"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Prediction on test set\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      outputs = model(b_input_ids, token_type_ids=None, \n","                      attention_mask=b_input_mask)\n","\n","  logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","print('    DONE.')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Predicting labels for 12,000 test sentences...\n","    DONE.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tN37q1Hdav6H","colab_type":"code","outputId":"c46e797c-06a7-44d5-d094-4a2a5c51ddae","executionInfo":{"status":"ok","timestamp":1586842895254,"user_tz":420,"elapsed":553,"user":{"displayName":"Zichuang Xie","photoUrl":"","userId":"08982348595392357027"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print('Positive samples: %d of %d (%.2f%%)' % (anger.anger.sum(), len(anger.anger), (anger.anger.sum() / len(anger.anger) * 100.0)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Positive samples: 479 of 12000 (3.99%)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YFvIcagGaz4O","colab_type":"code","outputId":"309fbb82-c6d2-4b55-85ee-4749fd53dd92","executionInfo":{"status":"ok","timestamp":1586842900313,"user_tz":420,"elapsed":1084,"user":{"displayName":"Zichuang Xie","photoUrl":"","userId":"08982348595392357027"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["from sklearn.metrics import matthews_corrcoef\n","\n","matthews_set = []\n","\n","# Evaluate each test batch using Matthew's correlation coefficient\n","print('Calculating Matthews Corr. Coef. for each batch...')\n","\n","# For each input batch...\n","for i in range(len(true_labels)):\n","  \n","  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n","  # and one column for \"1\"). Pick the label with the highest value and turn this\n","  # in to a list of 0s and 1s.\n","  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","  \n","  # Calculate and store the coef for this batch.  \n","  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n","  matthews_set.append(matthews)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Calculating Matthews Corr. Coef. for each batch...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n","  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"-U3k6ltEa65g","colab_type":"code","colab":{}},"source":["# Combine the predictions for each batch into a single list of 0s and 1s.\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","# Combine the correct labels for each batch into a single list.\n","flat_true_labels = [item for sublist in true_labels for item in sublist]\n","\n","# Calculate the MCC\n","mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n","\n","print('MCC: %.3f' % mcc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BwOqO5-hb5-L","colab_type":"text"},"source":["## Output Test Results"]},{"cell_type":"code","metadata":{"id":"_faiDsexc8vF","colab_type":"code","colab":{}},"source":["frames = [flat_true_labels, flat_predictions]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"for1e9pddC8d","colab_type":"code","colab":{}},"source":["table = pd.DataFrame(frames)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-2kTQoE6dGCZ","colab_type":"code","outputId":"8993067c-3dab-451f-b97c-eef7e63394d6","executionInfo":{"status":"ok","timestamp":1586813342184,"user_tz":420,"elapsed":993,"user":{"displayName":"Nan Luo","photoUrl":"","userId":"09863712867964797135"}},"colab":{"base_uri":"https://localhost:8080/","height":195}},"source":["results = table.T\n","results.columns =['True', 'Pred'] \n","results.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>True</th>\n","      <th>Pred</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   True  Pred\n","0     0     0\n","1     0     0\n","2     0     0\n","3     0     0\n","4     0     0"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"S_xHDbPAcQ_W","colab_type":"code","colab":{}},"source":["table_to_save = sadness.merge(results, left_index=True, right_index=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZA_4lcQb-qd","colab_type":"code","colab":{}},"source":["table_to_save.to_csv('testing_data_results.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IBKuOQ8_eY4F","colab_type":"text"},"source":["# Label Other Tweets"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"d1cbadbc-ab1d-455c-cdd7-73ea7cb4c2a8","executionInfo":{"status":"ok","timestamp":1586856122921,"user_tz":420,"elapsed":2513,"user":{"displayName":"Zichuang Xie","photoUrl":"","userId":"08982348595392357027"}},"id":"2O4kaGs8e8iL","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Load the dataset into a pandas dataframe.\n","df = pd.read_csv(\"/content/data_test_0308_0323.csv\")\n","\n","# Report the number of sentences.\n","print('Number of unlabeled sentences: {:,}\\n'.format(df.shape[0]))\n","analytical = df[['clean_text']]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of unlabeled sentences: 115,416\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9ADDNg_0ezb2","colab":{}},"source":["# Create sentence and label lists\n","sentences = analytical.clean_text.values\n","# labels = sadness.sadness.values\n","\n","# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","\n","# For every sentence...\n","for sent in sentences:\n","    encoded_sent = tokenizer.encode(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                   )\n","    \n","    input_ids.append(encoded_sent)\n","\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n","                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","# Convert to tensors.\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","# prediction_labels = torch.tensor(labels)\n","\n","# Set the batch size.  \n","batch_size = 16  \n","\n","# Create the DataLoader.\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"93796e4c-96bc-4c76-e88a-64019719d0b1","executionInfo":{"status":"ok","timestamp":1586851388621,"user_tz":420,"elapsed":1229971,"user":{"displayName":"Zichuang Xie","photoUrl":"","userId":"08982348595392357027"}},"id":"pMbfMQ2Fezb6","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Prediction on test set\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions  = []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      outputs = model(b_input_ids, token_type_ids=None, \n","                      attention_mask=b_input_mask)\n","\n","  logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  # label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  # true_labels.append(label_ids)\n","\n","print('    DONE.')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Predicting labels for 115,416 test sentences...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tpl0hjpqef1h","colab_type":"code","colab":{}},"source":["# Combine the predictions for each batch into a single list of 0s and 1s.\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vEykTu38k0GJ","colab_type":"code","colab":{}},"source":["table1 = pd.DataFrame(flat_predictions)\n","table1.columns = ['pred_anger']\n","df1 = df[['text', 'timestamp', 'clean_text']]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QOdrHDAa10-3","colab_type":"code","colab":{}},"source":["pred_output1 = df1.merge(table1,left_index=True, right_index=True)\n","pred_output1.to_csv('data_test_0324_0408_anger_lite.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F1z6Onqn-fGj","colab_type":"code","outputId":"63177346-6568-4c26-9ce9-604b450650a8","executionInfo":{"status":"ok","timestamp":1586851452224,"user_tz":420,"elapsed":378,"user":{"displayName":"Zichuang Xie","photoUrl":"","userId":"08982348595392357027"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["len(pred_output1)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["122541"]},"metadata":{"tags":[]},"execution_count":76}]}]}